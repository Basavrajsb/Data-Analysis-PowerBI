{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b66ea10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fcc13eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_confirmed=pd.read_csv('confirmed.csv')\n",
    "raw_deaths=pd.read_csv('deaths.csv')\n",
    "raw_recovered=pd.read_csv('recovered.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac3882f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(225, 50)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_recovered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcc2d441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Un-pivoting the data\n",
    "\n",
    "raw_data_confirmed = pd.melt(raw_confirmed, id_vars=['Province/State', 'Country/Region', 'Lat', 'Long'], var_name=['Date'])\n",
    "raw_data_deaths = pd.melt(raw_deaths, id_vars=['Province/State', 'Country/Region', 'Lat', 'Long'], var_name=['Date'])\n",
    "raw_data_Recovered = pd.melt(raw_recovered, id_vars=['Province/State', 'Country/Region', 'Lat', 'Long'], var_name=['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff95d308",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10350, 6)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data_confirmed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de4bfe68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Province/State</th>\n",
       "      <th>Country/Region</th>\n",
       "      <th>Lat</th>\n",
       "      <th>Long</th>\n",
       "      <th>Date</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Anhui</td>\n",
       "      <td>Mainland China</td>\n",
       "      <td>31.8257</td>\n",
       "      <td>117.2264</td>\n",
       "      <td>1/22/20</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Beijing</td>\n",
       "      <td>Mainland China</td>\n",
       "      <td>40.1824</td>\n",
       "      <td>116.4142</td>\n",
       "      <td>1/22/20</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Chongqing</td>\n",
       "      <td>Mainland China</td>\n",
       "      <td>30.0572</td>\n",
       "      <td>107.8740</td>\n",
       "      <td>1/22/20</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fujian</td>\n",
       "      <td>Mainland China</td>\n",
       "      <td>26.0789</td>\n",
       "      <td>117.9874</td>\n",
       "      <td>1/22/20</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gansu</td>\n",
       "      <td>Mainland China</td>\n",
       "      <td>36.0611</td>\n",
       "      <td>103.8343</td>\n",
       "      <td>1/22/20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Province/State  Country/Region      Lat      Long     Date  value\n",
       "0          Anhui  Mainland China  31.8257  117.2264  1/22/20      1\n",
       "1        Beijing  Mainland China  40.1824  116.4142  1/22/20     14\n",
       "2      Chongqing  Mainland China  30.0572  107.8740  1/22/20      6\n",
       "3         Fujian  Mainland China  26.0789  117.9874  1/22/20      1\n",
       "4          Gansu  Mainland China  36.0611  103.8343  1/22/20      0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data_confirmed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6a7f917",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_confirmed['Date'] = pd.to_datetime(raw_data_confirmed['Date'])\n",
    "raw_data_deaths['Date'] = pd.to_datetime(raw_data_deaths['Date'])\n",
    "raw_data_Recovered['Date'] = pd.to_datetime(raw_data_Recovered['Date'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9afd2b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming the Values column\n",
    "raw_data_confirmed.columns = raw_data_confirmed.columns.str.replace('value', 'Confirmed')\n",
    "raw_data_deaths.columns = raw_data_deaths.columns.str.replace('value', 'Deaths')\n",
    "raw_data_Recovered.columns = raw_data_Recovered.columns.str.replace('value', 'Recovered')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e80c5157",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Province/State</th>\n",
       "      <th>Country/Region</th>\n",
       "      <th>Lat</th>\n",
       "      <th>Long</th>\n",
       "      <th>Date</th>\n",
       "      <th>Confirmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Anhui</td>\n",
       "      <td>Mainland China</td>\n",
       "      <td>31.8257</td>\n",
       "      <td>117.2264</td>\n",
       "      <td>2020-01-22</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Beijing</td>\n",
       "      <td>Mainland China</td>\n",
       "      <td>40.1824</td>\n",
       "      <td>116.4142</td>\n",
       "      <td>2020-01-22</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Chongqing</td>\n",
       "      <td>Mainland China</td>\n",
       "      <td>30.0572</td>\n",
       "      <td>107.8740</td>\n",
       "      <td>2020-01-22</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fujian</td>\n",
       "      <td>Mainland China</td>\n",
       "      <td>26.0789</td>\n",
       "      <td>117.9874</td>\n",
       "      <td>2020-01-22</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gansu</td>\n",
       "      <td>Mainland China</td>\n",
       "      <td>36.0611</td>\n",
       "      <td>103.8343</td>\n",
       "      <td>2020-01-22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Province/State  Country/Region      Lat      Long       Date  Confirmed\n",
       "0          Anhui  Mainland China  31.8257  117.2264 2020-01-22          1\n",
       "1        Beijing  Mainland China  40.1824  116.4142 2020-01-22         14\n",
       "2      Chongqing  Mainland China  30.0572  107.8740 2020-01-22          6\n",
       "3         Fujian  Mainland China  26.0789  117.9874 2020-01-22          1\n",
       "4          Gansu  Mainland China  36.0611  103.8343 2020-01-22          0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data_confirmed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "840214e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Province/State    4324\n",
       "Country/Region       0\n",
       "Lat                  0\n",
       "Long                 0\n",
       "Date                 0\n",
       "Recovered            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data_Recovered.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53afdd33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Province/State    0\n",
       "Country/Region    0\n",
       "Lat               0\n",
       "Long              0\n",
       "Date              0\n",
       "Recovered         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  NULL values\n",
    "\n",
    "raw_data_confirmed['Province/State'].fillna(raw_data_confirmed['Country/Region'], inplace=True)\n",
    "raw_data_deaths['Province/State'].fillna(raw_data_deaths['Country/Region'], inplace=True)\n",
    "raw_data_Recovered['Province/State'].fillna(raw_data_Recovered['Country/Region'], inplace=True)\n",
    "\n",
    "raw_data_confirmed.isnull().sum()\n",
    "raw_data_deaths.isnull().sum()\n",
    "raw_data_Recovered.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7efad978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of first join:  (10350, 7)\n",
      "Shape of second join:  (10350, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Province/State</th>\n",
       "      <th>Country/Region</th>\n",
       "      <th>Lat</th>\n",
       "      <th>Long</th>\n",
       "      <th>Date</th>\n",
       "      <th>Confirmed</th>\n",
       "      <th>Deaths</th>\n",
       "      <th>Recovered</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Anhui</td>\n",
       "      <td>Mainland China</td>\n",
       "      <td>31.8257</td>\n",
       "      <td>117.2264</td>\n",
       "      <td>2020-01-22</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Beijing</td>\n",
       "      <td>Mainland China</td>\n",
       "      <td>40.1824</td>\n",
       "      <td>116.4142</td>\n",
       "      <td>2020-01-22</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Chongqing</td>\n",
       "      <td>Mainland China</td>\n",
       "      <td>30.0572</td>\n",
       "      <td>107.8740</td>\n",
       "      <td>2020-01-22</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fujian</td>\n",
       "      <td>Mainland China</td>\n",
       "      <td>26.0789</td>\n",
       "      <td>117.9874</td>\n",
       "      <td>2020-01-22</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gansu</td>\n",
       "      <td>Mainland China</td>\n",
       "      <td>36.0611</td>\n",
       "      <td>103.8343</td>\n",
       "      <td>2020-01-22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Province/State  Country/Region      Lat      Long       Date  Confirmed  \\\n",
       "0          Anhui  Mainland China  31.8257  117.2264 2020-01-22          1   \n",
       "1        Beijing  Mainland China  40.1824  116.4142 2020-01-22         14   \n",
       "2      Chongqing  Mainland China  30.0572  107.8740 2020-01-22          6   \n",
       "3         Fujian  Mainland China  26.0789  117.9874 2020-01-22          1   \n",
       "4          Gansu  Mainland China  36.0611  103.8343 2020-01-22          0   \n",
       "\n",
       "   Deaths  Recovered  \n",
       "0       0          0  \n",
       "1       0          0  \n",
       "2       0          0  \n",
       "3       0          0  \n",
       "4       0          0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#joining 3 tables\n",
    "\n",
    "# Confirmed with Deaths\n",
    "full_join = raw_data_confirmed.merge(raw_data_deaths[['Province/State','Country/Region','Date','Deaths']], \n",
    "                                      how = 'left', \n",
    "                                      left_on = ['Province/State','Country/Region','Date'], \n",
    "                                      right_on = ['Province/State', 'Country/Region','Date'])\n",
    "\n",
    "print(\"Shape of first join: \", full_join.shape)\n",
    "\n",
    "# full join with Recovered\n",
    "full_join = full_join.merge(raw_data_Recovered[['Province/State','Country/Region','Date','Recovered']], \n",
    "                                      how = 'left', \n",
    "                                      left_on = ['Province/State','Country/Region','Date'], \n",
    "                                      right_on = ['Province/State', 'Country/Region','Date'])\n",
    "\n",
    "print(\"Shape of second join: \", full_join.shape)\n",
    "\n",
    "full_join.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6e628fb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Province/State    0\n",
       "Country/Region    0\n",
       "Lat               0\n",
       "Long              0\n",
       "Date              0\n",
       "Confirmed         0\n",
       "Deaths            0\n",
       "Recovered         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_join.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0fe5de75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding Month and Year as a new Column\n",
    "full_join['Month-Year'] = full_join['Date'].dt.strftime('%b-%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8fd404e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Province/State</th>\n",
       "      <th>Country/Region</th>\n",
       "      <th>Lat</th>\n",
       "      <th>Long</th>\n",
       "      <th>Date</th>\n",
       "      <th>Confirmed</th>\n",
       "      <th>Deaths</th>\n",
       "      <th>Recovered</th>\n",
       "      <th>Month-Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Anhui</td>\n",
       "      <td>Mainland China</td>\n",
       "      <td>31.8257</td>\n",
       "      <td>117.2264</td>\n",
       "      <td>2020-01-22</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Jan-2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Beijing</td>\n",
       "      <td>Mainland China</td>\n",
       "      <td>40.1824</td>\n",
       "      <td>116.4142</td>\n",
       "      <td>2020-01-22</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Jan-2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Chongqing</td>\n",
       "      <td>Mainland China</td>\n",
       "      <td>30.0572</td>\n",
       "      <td>107.8740</td>\n",
       "      <td>2020-01-22</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Jan-2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fujian</td>\n",
       "      <td>Mainland China</td>\n",
       "      <td>26.0789</td>\n",
       "      <td>117.9874</td>\n",
       "      <td>2020-01-22</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Jan-2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gansu</td>\n",
       "      <td>Mainland China</td>\n",
       "      <td>36.0611</td>\n",
       "      <td>103.8343</td>\n",
       "      <td>2020-01-22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Jan-2020</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Province/State  Country/Region      Lat      Long       Date  Confirmed  \\\n",
       "0          Anhui  Mainland China  31.8257  117.2264 2020-01-22          1   \n",
       "1        Beijing  Mainland China  40.1824  116.4142 2020-01-22         14   \n",
       "2      Chongqing  Mainland China  30.0572  107.8740 2020-01-22          6   \n",
       "3         Fujian  Mainland China  26.0789  117.9874 2020-01-22          1   \n",
       "4          Gansu  Mainland China  36.0611  103.8343 2020-01-22          0   \n",
       "\n",
       "   Deaths  Recovered Month-Year  \n",
       "0       0          0   Jan-2020  \n",
       "1       0          0   Jan-2020  \n",
       "2       0          0   Jan-2020  \n",
       "3       0          0   Jan-2020  \n",
       "4       0          0   Jan-2020  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_join.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c6904c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Province/State</th>\n",
       "      <th>Country/Region</th>\n",
       "      <th>Lat</th>\n",
       "      <th>Long</th>\n",
       "      <th>Date</th>\n",
       "      <th>Confirmed</th>\n",
       "      <th>Deaths</th>\n",
       "      <th>Recovered</th>\n",
       "      <th>Month-Year</th>\n",
       "      <th>Confirmed - 1</th>\n",
       "      <th>Deaths - 1</th>\n",
       "      <th>Recovered - 1</th>\n",
       "      <th>Date - 1</th>\n",
       "      <th>Date Minus 1</th>\n",
       "      <th>Confirmed Daily</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Anhui</td>\n",
       "      <td>Mainland China</td>\n",
       "      <td>31.8257</td>\n",
       "      <td>117.2264</td>\n",
       "      <td>2020-01-22</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Jan-2020</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Anhui</td>\n",
       "      <td>Mainland China</td>\n",
       "      <td>31.8257</td>\n",
       "      <td>117.2264</td>\n",
       "      <td>2020-01-23</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Jan-2020</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2020-01-23</td>\n",
       "      <td>2020-01-22</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Anhui</td>\n",
       "      <td>Mainland China</td>\n",
       "      <td>31.8257</td>\n",
       "      <td>117.2264</td>\n",
       "      <td>2020-01-24</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Jan-2020</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2020-01-24</td>\n",
       "      <td>2020-01-23</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Anhui</td>\n",
       "      <td>Mainland China</td>\n",
       "      <td>31.8257</td>\n",
       "      <td>117.2264</td>\n",
       "      <td>2020-01-25</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Jan-2020</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2020-01-25</td>\n",
       "      <td>2020-01-24</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Anhui</td>\n",
       "      <td>Mainland China</td>\n",
       "      <td>31.8257</td>\n",
       "      <td>117.2264</td>\n",
       "      <td>2020-01-26</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Jan-2020</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2020-01-26</td>\n",
       "      <td>2020-01-25</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Province/State  Country/Region      Lat      Long       Date  Confirmed  \\\n",
       "0          Anhui  Mainland China  31.8257  117.2264 2020-01-22        1.0   \n",
       "1          Anhui  Mainland China  31.8257  117.2264 2020-01-23        9.0   \n",
       "2          Anhui  Mainland China  31.8257  117.2264 2020-01-24       15.0   \n",
       "3          Anhui  Mainland China  31.8257  117.2264 2020-01-25       39.0   \n",
       "4          Anhui  Mainland China  31.8257  117.2264 2020-01-26       60.0   \n",
       "\n",
       "   Deaths  Recovered Month-Year  Confirmed - 1  Deaths - 1  Recovered - 1  \\\n",
       "0     0.0        0.0   Jan-2020            NaN         NaN            NaN   \n",
       "1     0.0        0.0   Jan-2020            1.0         0.0            0.0   \n",
       "2     0.0        0.0   Jan-2020            9.0         0.0            0.0   \n",
       "3     0.0        0.0   Jan-2020           15.0         0.0            0.0   \n",
       "4     0.0        0.0   Jan-2020           39.0         0.0            0.0   \n",
       "\n",
       "    Date - 1 Date Minus 1  Confirmed Daily  \n",
       "0        NaT          NaT              NaN  \n",
       "1 2020-01-23   2020-01-22              8.0  \n",
       "2 2020-01-24   2020-01-23              6.0  \n",
       "3 2020-01-25   2020-01-24             24.0  \n",
       "4 2020-01-26   2020-01-25             21.0  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating a new df    \n",
    "test = full_join[full_join['Province/State'] == 'Anhui']\n",
    "\n",
    "#creating a new df    \n",
    "full_join2 = test.copy()\n",
    "\n",
    "#creating a new date columns - 1\n",
    "full_join2['Date - 1'] = full_join2['Date'] + pd.Timedelta(days=1)\n",
    "full_join2.rename(columns={'Confirmed': 'Confirmed - 1', 'Deaths': 'Deaths - 1', 'Recovered': 'Recovered - 1',\n",
    "                          'Date': 'Date Minus 1'}, inplace=True)\n",
    "\n",
    "#Joing on the 2 DFs\n",
    "full_join3 = test.merge(full_join2[['Province/State', 'Country/Region','Confirmed - 1', 'Deaths - 1', \n",
    "                            'Recovered - 1', 'Date - 1', 'Date Minus 1']], how = 'outer',\n",
    "                             left_on = ['Province/State','Country/Region','Date'], \n",
    "                             right_on = ['Province/State', 'Country/Region','Date - 1'])\n",
    "\n",
    "# Additional Calculations\n",
    "full_join3['Confirmed Daily'] = full_join3['Confirmed'] - full_join3['Confirmed - 1']\n",
    "\n",
    "\n",
    "test.head()\n",
    "full_join2.head()\n",
    "full_join3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7053741f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Province/State</th>\n",
       "      <th>Country/Region</th>\n",
       "      <th>Lat</th>\n",
       "      <th>Long</th>\n",
       "      <th>Date</th>\n",
       "      <th>Confirmed</th>\n",
       "      <th>Deaths</th>\n",
       "      <th>Recovered</th>\n",
       "      <th>Month-Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Anhui</td>\n",
       "      <td>Mainland China</td>\n",
       "      <td>31.8257</td>\n",
       "      <td>117.2264</td>\n",
       "      <td>2020-01-22</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Jan-2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>Anhui</td>\n",
       "      <td>Mainland China</td>\n",
       "      <td>31.8257</td>\n",
       "      <td>117.2264</td>\n",
       "      <td>2020-01-23</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Jan-2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450</th>\n",
       "      <td>Anhui</td>\n",
       "      <td>Mainland China</td>\n",
       "      <td>31.8257</td>\n",
       "      <td>117.2264</td>\n",
       "      <td>2020-01-24</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Jan-2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>675</th>\n",
       "      <td>Anhui</td>\n",
       "      <td>Mainland China</td>\n",
       "      <td>31.8257</td>\n",
       "      <td>117.2264</td>\n",
       "      <td>2020-01-25</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Jan-2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>900</th>\n",
       "      <td>Anhui</td>\n",
       "      <td>Mainland China</td>\n",
       "      <td>31.8257</td>\n",
       "      <td>117.2264</td>\n",
       "      <td>2020-01-26</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Jan-2020</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Province/State  Country/Region      Lat      Long       Date  Confirmed  \\\n",
       "0            Anhui  Mainland China  31.8257  117.2264 2020-01-22          1   \n",
       "225          Anhui  Mainland China  31.8257  117.2264 2020-01-23          9   \n",
       "450          Anhui  Mainland China  31.8257  117.2264 2020-01-24         15   \n",
       "675          Anhui  Mainland China  31.8257  117.2264 2020-01-25         39   \n",
       "900          Anhui  Mainland China  31.8257  117.2264 2020-01-26         60   \n",
       "\n",
       "     Deaths  Recovered Month-Year  \n",
       "0         0          0   Jan-2020  \n",
       "225       0          0   Jan-2020  \n",
       "450       0          0   Jan-2020  \n",
       "675       0          0   Jan-2020  \n",
       "900       0          0   Jan-2020  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6455e3ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Province/State</th>\n",
       "      <th>Country/Region</th>\n",
       "      <th>Lat</th>\n",
       "      <th>Long</th>\n",
       "      <th>Date Minus 1</th>\n",
       "      <th>Confirmed - 1</th>\n",
       "      <th>Deaths - 1</th>\n",
       "      <th>Recovered - 1</th>\n",
       "      <th>Month-Year</th>\n",
       "      <th>Date - 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Anhui</td>\n",
       "      <td>Mainland China</td>\n",
       "      <td>31.8257</td>\n",
       "      <td>117.2264</td>\n",
       "      <td>2020-01-22</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Jan-2020</td>\n",
       "      <td>2020-01-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>Anhui</td>\n",
       "      <td>Mainland China</td>\n",
       "      <td>31.8257</td>\n",
       "      <td>117.2264</td>\n",
       "      <td>2020-01-23</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Jan-2020</td>\n",
       "      <td>2020-01-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450</th>\n",
       "      <td>Anhui</td>\n",
       "      <td>Mainland China</td>\n",
       "      <td>31.8257</td>\n",
       "      <td>117.2264</td>\n",
       "      <td>2020-01-24</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Jan-2020</td>\n",
       "      <td>2020-01-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>675</th>\n",
       "      <td>Anhui</td>\n",
       "      <td>Mainland China</td>\n",
       "      <td>31.8257</td>\n",
       "      <td>117.2264</td>\n",
       "      <td>2020-01-25</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Jan-2020</td>\n",
       "      <td>2020-01-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>900</th>\n",
       "      <td>Anhui</td>\n",
       "      <td>Mainland China</td>\n",
       "      <td>31.8257</td>\n",
       "      <td>117.2264</td>\n",
       "      <td>2020-01-26</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Jan-2020</td>\n",
       "      <td>2020-01-27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Province/State  Country/Region      Lat      Long Date Minus 1  \\\n",
       "0            Anhui  Mainland China  31.8257  117.2264   2020-01-22   \n",
       "225          Anhui  Mainland China  31.8257  117.2264   2020-01-23   \n",
       "450          Anhui  Mainland China  31.8257  117.2264   2020-01-24   \n",
       "675          Anhui  Mainland China  31.8257  117.2264   2020-01-25   \n",
       "900          Anhui  Mainland China  31.8257  117.2264   2020-01-26   \n",
       "\n",
       "     Confirmed - 1  Deaths - 1  Recovered - 1 Month-Year   Date - 1  \n",
       "0                1           0              0   Jan-2020 2020-01-23  \n",
       "225              9           0              0   Jan-2020 2020-01-24  \n",
       "450             15           0              0   Jan-2020 2020-01-25  \n",
       "675             39           0              0   Jan-2020 2020-01-26  \n",
       "900             60           0              0   Jan-2020 2020-01-27  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_join2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2ecfc929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10350, 17)\n"
     ]
    }
   ],
   "source": [
    "## Applying it on all dataset\n",
    "\n",
    "#creating a new df    \n",
    "full_join2 = full_join.copy()\n",
    "\n",
    "#creating a new date columns - 1\n",
    "full_join2['Date - 1'] = full_join2['Date'] + pd.Timedelta(days=1)\n",
    "full_join2.rename(columns={'Confirmed': 'Confirmed - 1', 'Deaths': 'Deaths - 1', 'Recovered': 'Recovered - 1',\n",
    "                          'Date': 'Date Minus 1'}, inplace=True)\n",
    "\n",
    "#Joing on the 2 DFs\n",
    "full_join3 = full_join.merge(full_join2[['Province/State', 'Country/Region','Confirmed - 1', 'Deaths - 1', \n",
    "                            'Recovered - 1', 'Date - 1', 'Date Minus 1']], how = 'left',\n",
    "                             left_on = ['Province/State','Country/Region','Date'], \n",
    "                             right_on = ['Province/State', 'Country/Region','Date - 1'])\n",
    "\n",
    "#minus_onedf.rename(columns={'Confirmed': 'Confirmed - 1', 'Deaths': 'Deaths - 1', 'Recovered': 'Recovered - 1'}, inplace=True)\n",
    "\n",
    "full_join3.head()\n",
    "\n",
    "# Additional Calculations\n",
    "full_join3['Confirmed Daily'] = full_join3['Confirmed'] - full_join3['Confirmed - 1']\n",
    "full_join3['Deaths Daily'] = full_join3['Deaths'] - full_join3['Deaths - 1']\n",
    "full_join3['Recovered Daily'] = full_join3['Recovered'] - full_join3['Recovered - 1']\n",
    "\n",
    "print(full_join3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "992e7494",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Province/State</th>\n",
       "      <th>Country/Region</th>\n",
       "      <th>Lat</th>\n",
       "      <th>Long</th>\n",
       "      <th>Date</th>\n",
       "      <th>Confirmed</th>\n",
       "      <th>Deaths</th>\n",
       "      <th>Recovered</th>\n",
       "      <th>Month-Year</th>\n",
       "      <th>Confirmed - 1</th>\n",
       "      <th>Deaths - 1</th>\n",
       "      <th>Recovered - 1</th>\n",
       "      <th>Date - 1</th>\n",
       "      <th>Date Minus 1</th>\n",
       "      <th>Confirmed Daily</th>\n",
       "      <th>Deaths Daily</th>\n",
       "      <th>Recovered Daily</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Anhui</td>\n",
       "      <td>Mainland China</td>\n",
       "      <td>31.8257</td>\n",
       "      <td>117.2264</td>\n",
       "      <td>2020-01-22</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Jan-2020</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Beijing</td>\n",
       "      <td>Mainland China</td>\n",
       "      <td>40.1824</td>\n",
       "      <td>116.4142</td>\n",
       "      <td>2020-01-22</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Jan-2020</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Chongqing</td>\n",
       "      <td>Mainland China</td>\n",
       "      <td>30.0572</td>\n",
       "      <td>107.8740</td>\n",
       "      <td>2020-01-22</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Jan-2020</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fujian</td>\n",
       "      <td>Mainland China</td>\n",
       "      <td>26.0789</td>\n",
       "      <td>117.9874</td>\n",
       "      <td>2020-01-22</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Jan-2020</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gansu</td>\n",
       "      <td>Mainland China</td>\n",
       "      <td>36.0611</td>\n",
       "      <td>103.8343</td>\n",
       "      <td>2020-01-22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Jan-2020</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Province/State  Country/Region      Lat      Long       Date  Confirmed  \\\n",
       "0          Anhui  Mainland China  31.8257  117.2264 2020-01-22          1   \n",
       "1        Beijing  Mainland China  40.1824  116.4142 2020-01-22         14   \n",
       "2      Chongqing  Mainland China  30.0572  107.8740 2020-01-22          6   \n",
       "3         Fujian  Mainland China  26.0789  117.9874 2020-01-22          1   \n",
       "4          Gansu  Mainland China  36.0611  103.8343 2020-01-22          0   \n",
       "\n",
       "   Deaths  Recovered Month-Year  Confirmed - 1  Deaths - 1  Recovered - 1  \\\n",
       "0       0          0   Jan-2020            NaN         NaN            NaN   \n",
       "1       0          0   Jan-2020            NaN         NaN            NaN   \n",
       "2       0          0   Jan-2020            NaN         NaN            NaN   \n",
       "3       0          0   Jan-2020            NaN         NaN            NaN   \n",
       "4       0          0   Jan-2020            NaN         NaN            NaN   \n",
       "\n",
       "  Date - 1 Date Minus 1  Confirmed Daily  Deaths Daily  Recovered Daily  \n",
       "0      NaT          NaT              NaN           NaN              NaN  \n",
       "1      NaT          NaT              NaN           NaN              NaN  \n",
       "2      NaT          NaT              NaN           NaN              NaN  \n",
       "3      NaT          NaT              NaN           NaN              NaN  \n",
       "4      NaT          NaT              NaN           NaN              NaN  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_join3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4389876c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_16288\\2528994319.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  full_join3['Confirmed Daily'].loc[full_join3['Date'] == '2020-01-22'] = full_join3['Confirmed']\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_16288\\2528994319.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  full_join3['Deaths Daily'].loc[full_join3['Date'] == '2020-01-22'] = full_join3['Deaths']\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_16288\\2528994319.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  full_join3['Recovered Daily'].loc[full_join3['Date'] == '2020-01-22'] = full_join3['Recovered']\n"
     ]
    }
   ],
   "source": [
    "# Additing manually the numbers for first day\n",
    "\n",
    "full_join3['Confirmed Daily'].loc[full_join3['Date'] == '2020-01-22'] = full_join3['Confirmed']\n",
    "full_join3['Deaths Daily'].loc[full_join3['Date'] == '2020-01-22'] = full_join3['Deaths']\n",
    "full_join3['Recovered Daily'].loc[full_join3['Date'] == '2020-01-22'] = full_join3['Recovered']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6c26870f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deleting columns\n",
    "del full_join3['Confirmed - 1']\n",
    "del full_join3['Deaths - 1']\n",
    "del full_join3['Recovered - 1']\n",
    "del full_join3['Date - 1']\n",
    "del full_join3['Date Minus 1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "43f0a551",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Province/State</th>\n",
       "      <th>Country/Region</th>\n",
       "      <th>Lat</th>\n",
       "      <th>Long</th>\n",
       "      <th>Date</th>\n",
       "      <th>Confirmed</th>\n",
       "      <th>Deaths</th>\n",
       "      <th>Recovered</th>\n",
       "      <th>Month-Year</th>\n",
       "      <th>Confirmed Daily</th>\n",
       "      <th>Deaths Daily</th>\n",
       "      <th>Recovered Daily</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Anhui</td>\n",
       "      <td>Mainland China</td>\n",
       "      <td>31.8257</td>\n",
       "      <td>117.2264</td>\n",
       "      <td>2020-01-22</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Jan-2020</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Beijing</td>\n",
       "      <td>Mainland China</td>\n",
       "      <td>40.1824</td>\n",
       "      <td>116.4142</td>\n",
       "      <td>2020-01-22</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Jan-2020</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Chongqing</td>\n",
       "      <td>Mainland China</td>\n",
       "      <td>30.0572</td>\n",
       "      <td>107.8740</td>\n",
       "      <td>2020-01-22</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Jan-2020</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fujian</td>\n",
       "      <td>Mainland China</td>\n",
       "      <td>26.0789</td>\n",
       "      <td>117.9874</td>\n",
       "      <td>2020-01-22</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Jan-2020</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gansu</td>\n",
       "      <td>Mainland China</td>\n",
       "      <td>36.0611</td>\n",
       "      <td>103.8343</td>\n",
       "      <td>2020-01-22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Jan-2020</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10345</th>\n",
       "      <td>Pierce County, WA</td>\n",
       "      <td>US</td>\n",
       "      <td>47.0676</td>\n",
       "      <td>-122.1295</td>\n",
       "      <td>2020-03-07</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Mar-2020</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10346</th>\n",
       "      <td>Plymouth County, MA</td>\n",
       "      <td>US</td>\n",
       "      <td>42.1615</td>\n",
       "      <td>-70.7928</td>\n",
       "      <td>2020-03-07</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Mar-2020</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10347</th>\n",
       "      <td>Santa Cruz County, CA</td>\n",
       "      <td>US</td>\n",
       "      <td>36.9741</td>\n",
       "      <td>-122.0308</td>\n",
       "      <td>2020-03-07</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Mar-2020</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10348</th>\n",
       "      <td>Tulsa County, OK</td>\n",
       "      <td>US</td>\n",
       "      <td>36.1593</td>\n",
       "      <td>-95.9410</td>\n",
       "      <td>2020-03-07</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Mar-2020</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10349</th>\n",
       "      <td>Montgomery County, TX</td>\n",
       "      <td>US</td>\n",
       "      <td>30.3213</td>\n",
       "      <td>-95.4778</td>\n",
       "      <td>2020-03-07</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Mar-2020</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10350 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Province/State  Country/Region      Lat      Long       Date  \\\n",
       "0                      Anhui  Mainland China  31.8257  117.2264 2020-01-22   \n",
       "1                    Beijing  Mainland China  40.1824  116.4142 2020-01-22   \n",
       "2                  Chongqing  Mainland China  30.0572  107.8740 2020-01-22   \n",
       "3                     Fujian  Mainland China  26.0789  117.9874 2020-01-22   \n",
       "4                      Gansu  Mainland China  36.0611  103.8343 2020-01-22   \n",
       "...                      ...             ...      ...       ...        ...   \n",
       "10345      Pierce County, WA              US  47.0676 -122.1295 2020-03-07   \n",
       "10346    Plymouth County, MA              US  42.1615  -70.7928 2020-03-07   \n",
       "10347  Santa Cruz County, CA              US  36.9741 -122.0308 2020-03-07   \n",
       "10348       Tulsa County, OK              US  36.1593  -95.9410 2020-03-07   \n",
       "10349  Montgomery County, TX              US  30.3213  -95.4778 2020-03-07   \n",
       "\n",
       "       Confirmed  Deaths  Recovered Month-Year  Confirmed Daily  Deaths Daily  \\\n",
       "0              1       0          0   Jan-2020              1.0           0.0   \n",
       "1             14       0          0   Jan-2020             14.0           0.0   \n",
       "2              6       0          0   Jan-2020              6.0           0.0   \n",
       "3              1       0          0   Jan-2020              1.0           0.0   \n",
       "4              0       0          0   Jan-2020              0.0           0.0   \n",
       "...          ...     ...        ...        ...              ...           ...   \n",
       "10345          1       0          0   Mar-2020              1.0           0.0   \n",
       "10346          1       0          0   Mar-2020              0.0           0.0   \n",
       "10347          1       0          0   Mar-2020              1.0           0.0   \n",
       "10348          1       0          0   Mar-2020              1.0           0.0   \n",
       "10349          0       0          0   Mar-2020              0.0           0.0   \n",
       "\n",
       "       Recovered Daily  \n",
       "0                  0.0  \n",
       "1                  0.0  \n",
       "2                  0.0  \n",
       "3                  0.0  \n",
       "4                  0.0  \n",
       "...                ...  \n",
       "10345              0.0  \n",
       "10346              0.0  \n",
       "10347              0.0  \n",
       "10348              0.0  \n",
       "10349              0.0  \n",
       "\n",
       "[10350 rows x 12 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_join3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a60149d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_16288\\1742397093.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  full_join3['Hubei Vs Rest of the World'].loc[full_join3['Province/State'] == 'Hubei'] = 'Hubei - Virus birth'\n"
     ]
    }
   ],
   "source": [
    "# Creating additional slicer for easy of use\n",
    "\n",
    "full_join3['Hubei Vs Rest of the World'] = 'Rest of the World'\n",
    "full_join3['Hubei Vs Rest of the World'].loc[full_join3['Province/State'] == 'Hubei'] = 'Hubei - Virus birth'\n",
    "\n",
    "#full_join3[full_join3['Province/State'] == 'Hubei']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "18e47985",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Province/State</th>\n",
       "      <th>Country/Region</th>\n",
       "      <th>Lat</th>\n",
       "      <th>Long</th>\n",
       "      <th>Date</th>\n",
       "      <th>Confirmed</th>\n",
       "      <th>Deaths</th>\n",
       "      <th>Recovered</th>\n",
       "      <th>Month-Year</th>\n",
       "      <th>Confirmed Daily</th>\n",
       "      <th>Deaths Daily</th>\n",
       "      <th>Recovered Daily</th>\n",
       "      <th>Hubei Vs Rest of the World</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Anhui</td>\n",
       "      <td>Mainland China</td>\n",
       "      <td>31.8257</td>\n",
       "      <td>117.2264</td>\n",
       "      <td>2020-01-22</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Jan-2020</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Rest of the World</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Beijing</td>\n",
       "      <td>Mainland China</td>\n",
       "      <td>40.1824</td>\n",
       "      <td>116.4142</td>\n",
       "      <td>2020-01-22</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Jan-2020</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Rest of the World</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Chongqing</td>\n",
       "      <td>Mainland China</td>\n",
       "      <td>30.0572</td>\n",
       "      <td>107.8740</td>\n",
       "      <td>2020-01-22</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Jan-2020</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Rest of the World</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fujian</td>\n",
       "      <td>Mainland China</td>\n",
       "      <td>26.0789</td>\n",
       "      <td>117.9874</td>\n",
       "      <td>2020-01-22</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Jan-2020</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Rest of the World</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gansu</td>\n",
       "      <td>Mainland China</td>\n",
       "      <td>36.0611</td>\n",
       "      <td>103.8343</td>\n",
       "      <td>2020-01-22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Jan-2020</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Rest of the World</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Province/State  Country/Region      Lat      Long       Date  Confirmed  \\\n",
       "0          Anhui  Mainland China  31.8257  117.2264 2020-01-22          1   \n",
       "1        Beijing  Mainland China  40.1824  116.4142 2020-01-22         14   \n",
       "2      Chongqing  Mainland China  30.0572  107.8740 2020-01-22          6   \n",
       "3         Fujian  Mainland China  26.0789  117.9874 2020-01-22          1   \n",
       "4          Gansu  Mainland China  36.0611  103.8343 2020-01-22          0   \n",
       "\n",
       "   Deaths  Recovered Month-Year  Confirmed Daily  Deaths Daily  \\\n",
       "0       0          0   Jan-2020              1.0           0.0   \n",
       "1       0          0   Jan-2020             14.0           0.0   \n",
       "2       0          0   Jan-2020              6.0           0.0   \n",
       "3       0          0   Jan-2020              1.0           0.0   \n",
       "4       0          0   Jan-2020              0.0           0.0   \n",
       "\n",
       "   Recovered Daily Hubei Vs Rest of the World  \n",
       "0              0.0          Rest of the World  \n",
       "1              0.0          Rest of the World  \n",
       "2              0.0          Rest of the World  \n",
       "3              0.0          Rest of the World  \n",
       "4              0.0          Rest of the World  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_join3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4cba8260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting the data\n",
    "\n",
    "# Setting my path\n",
    "path = \"C:\\\\Users\\\\Admin\\\\Desktop\\\\powerbi\\\\covid-19\"\n",
    "\n",
    "# Changing my CWD\n",
    "os.chdir(path)\n",
    "\n",
    "full_join3.to_csv('CoronaVirus PowerBI Raw', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "77deb429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10350, 13)\n",
      "\n",
      "\n",
      "2020-01-22T00:00:00.000000000\n",
      "2020-01-23T00:00:00.000000000\n",
      "['2020-01-22T00:00:00.000000000' '2020-01-23T00:00:00.000000000']\n",
      "2020-01-24T00:00:00.000000000\n",
      "['2020-01-22T00:00:00.000000000' '2020-01-23T00:00:00.000000000'\n",
      " '2020-01-24T00:00:00.000000000']\n",
      "2020-01-25T00:00:00.000000000\n",
      "['2020-01-22T00:00:00.000000000' '2020-01-23T00:00:00.000000000'\n",
      " '2020-01-24T00:00:00.000000000' '2020-01-25T00:00:00.000000000']\n",
      "2020-01-26T00:00:00.000000000\n",
      "['2020-01-22T00:00:00.000000000' '2020-01-23T00:00:00.000000000'\n",
      " '2020-01-24T00:00:00.000000000' '2020-01-25T00:00:00.000000000'\n",
      " '2020-01-26T00:00:00.000000000']\n",
      "2020-01-27T00:00:00.000000000\n",
      "['2020-01-22T00:00:00.000000000' '2020-01-23T00:00:00.000000000'\n",
      " '2020-01-24T00:00:00.000000000' '2020-01-25T00:00:00.000000000'\n",
      " '2020-01-26T00:00:00.000000000' '2020-01-27T00:00:00.000000000']\n",
      "2020-01-28T00:00:00.000000000\n",
      "['2020-01-22T00:00:00.000000000' '2020-01-23T00:00:00.000000000'\n",
      " '2020-01-24T00:00:00.000000000' '2020-01-25T00:00:00.000000000'\n",
      " '2020-01-26T00:00:00.000000000' '2020-01-27T00:00:00.000000000'\n",
      " '2020-01-28T00:00:00.000000000']\n",
      "2020-01-29T00:00:00.000000000\n",
      "['2020-01-22T00:00:00.000000000' '2020-01-23T00:00:00.000000000'\n",
      " '2020-01-24T00:00:00.000000000' '2020-01-25T00:00:00.000000000'\n",
      " '2020-01-26T00:00:00.000000000' '2020-01-27T00:00:00.000000000'\n",
      " '2020-01-28T00:00:00.000000000' '2020-01-29T00:00:00.000000000']\n",
      "2020-01-30T00:00:00.000000000\n",
      "['2020-01-22T00:00:00.000000000' '2020-01-23T00:00:00.000000000'\n",
      " '2020-01-24T00:00:00.000000000' '2020-01-25T00:00:00.000000000'\n",
      " '2020-01-26T00:00:00.000000000' '2020-01-27T00:00:00.000000000'\n",
      " '2020-01-28T00:00:00.000000000' '2020-01-29T00:00:00.000000000'\n",
      " '2020-01-30T00:00:00.000000000']\n",
      "2020-01-31T00:00:00.000000000\n",
      "['2020-01-22T00:00:00.000000000' '2020-01-23T00:00:00.000000000'\n",
      " '2020-01-24T00:00:00.000000000' '2020-01-25T00:00:00.000000000'\n",
      " '2020-01-26T00:00:00.000000000' '2020-01-27T00:00:00.000000000'\n",
      " '2020-01-28T00:00:00.000000000' '2020-01-29T00:00:00.000000000'\n",
      " '2020-01-30T00:00:00.000000000' '2020-01-31T00:00:00.000000000']\n",
      "2020-02-01T00:00:00.000000000\n",
      "['2020-01-22T00:00:00.000000000' '2020-01-23T00:00:00.000000000'\n",
      " '2020-01-24T00:00:00.000000000' '2020-01-25T00:00:00.000000000'\n",
      " '2020-01-26T00:00:00.000000000' '2020-01-27T00:00:00.000000000'\n",
      " '2020-01-28T00:00:00.000000000' '2020-01-29T00:00:00.000000000'\n",
      " '2020-01-30T00:00:00.000000000' '2020-01-31T00:00:00.000000000'\n",
      " '2020-02-01T00:00:00.000000000']\n",
      "2020-02-02T00:00:00.000000000\n",
      "['2020-01-22T00:00:00.000000000' '2020-01-23T00:00:00.000000000'\n",
      " '2020-01-24T00:00:00.000000000' '2020-01-25T00:00:00.000000000'\n",
      " '2020-01-26T00:00:00.000000000' '2020-01-27T00:00:00.000000000'\n",
      " '2020-01-28T00:00:00.000000000' '2020-01-29T00:00:00.000000000'\n",
      " '2020-01-30T00:00:00.000000000' '2020-01-31T00:00:00.000000000'\n",
      " '2020-02-01T00:00:00.000000000' '2020-02-02T00:00:00.000000000']\n",
      "2020-02-03T00:00:00.000000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_16288\\3429718920.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_data['Cumulative Date'] = i\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_16288\\3429718920.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_data['Cumulative Date'] = i\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_16288\\3429718920.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_data['Cumulative Date'] = i\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_16288\\3429718920.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_data['Cumulative Date'] = i\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_16288\\3429718920.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_data['Cumulative Date'] = i\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_16288\\3429718920.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_data['Cumulative Date'] = i\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_16288\\3429718920.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_data['Cumulative Date'] = i\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_16288\\3429718920.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_data['Cumulative Date'] = i\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_16288\\3429718920.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_data['Cumulative Date'] = i\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_16288\\3429718920.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_data['Cumulative Date'] = i\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_16288\\3429718920.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_data['Cumulative Date'] = i\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_16288\\3429718920.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_data['Cumulative Date'] = i\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_16288\\3429718920.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_data['Cumulative Date'] = i\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_16288\\3429718920.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_data['Cumulative Date'] = i\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_16288\\3429718920.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_data['Cumulative Date'] = i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2020-01-22T00:00:00.000000000' '2020-01-23T00:00:00.000000000'\n",
      " '2020-01-24T00:00:00.000000000' '2020-01-25T00:00:00.000000000'\n",
      " '2020-01-26T00:00:00.000000000' '2020-01-27T00:00:00.000000000'\n",
      " '2020-01-28T00:00:00.000000000' '2020-01-29T00:00:00.000000000'\n",
      " '2020-01-30T00:00:00.000000000' '2020-01-31T00:00:00.000000000'\n",
      " '2020-02-01T00:00:00.000000000' '2020-02-02T00:00:00.000000000'\n",
      " '2020-02-03T00:00:00.000000000']\n",
      "2020-02-04T00:00:00.000000000\n",
      "['2020-01-22T00:00:00.000000000' '2020-01-23T00:00:00.000000000'\n",
      " '2020-01-24T00:00:00.000000000' '2020-01-25T00:00:00.000000000'\n",
      " '2020-01-26T00:00:00.000000000' '2020-01-27T00:00:00.000000000'\n",
      " '2020-01-28T00:00:00.000000000' '2020-01-29T00:00:00.000000000'\n",
      " '2020-01-30T00:00:00.000000000' '2020-01-31T00:00:00.000000000'\n",
      " '2020-02-01T00:00:00.000000000' '2020-02-02T00:00:00.000000000'\n",
      " '2020-02-03T00:00:00.000000000' '2020-02-04T00:00:00.000000000']\n",
      "2020-02-05T00:00:00.000000000\n",
      "['2020-01-22T00:00:00.000000000' '2020-01-23T00:00:00.000000000'\n",
      " '2020-01-24T00:00:00.000000000' '2020-01-25T00:00:00.000000000'\n",
      " '2020-01-26T00:00:00.000000000' '2020-01-27T00:00:00.000000000'\n",
      " '2020-01-28T00:00:00.000000000' '2020-01-29T00:00:00.000000000'\n",
      " '2020-01-30T00:00:00.000000000' '2020-01-31T00:00:00.000000000'\n",
      " '2020-02-01T00:00:00.000000000' '2020-02-02T00:00:00.000000000'\n",
      " '2020-02-03T00:00:00.000000000' '2020-02-04T00:00:00.000000000'\n",
      " '2020-02-05T00:00:00.000000000']\n",
      "2020-02-06T00:00:00.000000000\n",
      "['2020-01-22T00:00:00.000000000' '2020-01-23T00:00:00.000000000'\n",
      " '2020-01-24T00:00:00.000000000' '2020-01-25T00:00:00.000000000'\n",
      " '2020-01-26T00:00:00.000000000' '2020-01-27T00:00:00.000000000'\n",
      " '2020-01-28T00:00:00.000000000' '2020-01-29T00:00:00.000000000'\n",
      " '2020-01-30T00:00:00.000000000' '2020-01-31T00:00:00.000000000'\n",
      " '2020-02-01T00:00:00.000000000' '2020-02-02T00:00:00.000000000'\n",
      " '2020-02-03T00:00:00.000000000' '2020-02-04T00:00:00.000000000'\n",
      " '2020-02-05T00:00:00.000000000' '2020-02-06T00:00:00.000000000']\n",
      "2020-02-07T00:00:00.000000000\n",
      "['2020-01-22T00:00:00.000000000' '2020-01-23T00:00:00.000000000'\n",
      " '2020-01-24T00:00:00.000000000' '2020-01-25T00:00:00.000000000'\n",
      " '2020-01-26T00:00:00.000000000' '2020-01-27T00:00:00.000000000'\n",
      " '2020-01-28T00:00:00.000000000' '2020-01-29T00:00:00.000000000'\n",
      " '2020-01-30T00:00:00.000000000' '2020-01-31T00:00:00.000000000'\n",
      " '2020-02-01T00:00:00.000000000' '2020-02-02T00:00:00.000000000'\n",
      " '2020-02-03T00:00:00.000000000' '2020-02-04T00:00:00.000000000'\n",
      " '2020-02-05T00:00:00.000000000' '2020-02-06T00:00:00.000000000'\n",
      " '2020-02-07T00:00:00.000000000']\n",
      "2020-02-08T00:00:00.000000000\n",
      "['2020-01-22T00:00:00.000000000' '2020-01-23T00:00:00.000000000'\n",
      " '2020-01-24T00:00:00.000000000' '2020-01-25T00:00:00.000000000'\n",
      " '2020-01-26T00:00:00.000000000' '2020-01-27T00:00:00.000000000'\n",
      " '2020-01-28T00:00:00.000000000' '2020-01-29T00:00:00.000000000'\n",
      " '2020-01-30T00:00:00.000000000' '2020-01-31T00:00:00.000000000'\n",
      " '2020-02-01T00:00:00.000000000' '2020-02-02T00:00:00.000000000'\n",
      " '2020-02-03T00:00:00.000000000' '2020-02-04T00:00:00.000000000'\n",
      " '2020-02-05T00:00:00.000000000' '2020-02-06T00:00:00.000000000'\n",
      " '2020-02-07T00:00:00.000000000' '2020-02-08T00:00:00.000000000']\n",
      "2020-02-09T00:00:00.000000000\n",
      "['2020-01-22T00:00:00.000000000' '2020-01-23T00:00:00.000000000'\n",
      " '2020-01-24T00:00:00.000000000' '2020-01-25T00:00:00.000000000'\n",
      " '2020-01-26T00:00:00.000000000' '2020-01-27T00:00:00.000000000'\n",
      " '2020-01-28T00:00:00.000000000' '2020-01-29T00:00:00.000000000'\n",
      " '2020-01-30T00:00:00.000000000' '2020-01-31T00:00:00.000000000'\n",
      " '2020-02-01T00:00:00.000000000' '2020-02-02T00:00:00.000000000'\n",
      " '2020-02-03T00:00:00.000000000' '2020-02-04T00:00:00.000000000'\n",
      " '2020-02-05T00:00:00.000000000' '2020-02-06T00:00:00.000000000'\n",
      " '2020-02-07T00:00:00.000000000' '2020-02-08T00:00:00.000000000'\n",
      " '2020-02-09T00:00:00.000000000']\n",
      "2020-02-10T00:00:00.000000000\n",
      "['2020-01-22T00:00:00.000000000' '2020-01-23T00:00:00.000000000'\n",
      " '2020-01-24T00:00:00.000000000' '2020-01-25T00:00:00.000000000'\n",
      " '2020-01-26T00:00:00.000000000' '2020-01-27T00:00:00.000000000'\n",
      " '2020-01-28T00:00:00.000000000' '2020-01-29T00:00:00.000000000'\n",
      " '2020-01-30T00:00:00.000000000' '2020-01-31T00:00:00.000000000'\n",
      " '2020-02-01T00:00:00.000000000' '2020-02-02T00:00:00.000000000'\n",
      " '2020-02-03T00:00:00.000000000' '2020-02-04T00:00:00.000000000'\n",
      " '2020-02-05T00:00:00.000000000' '2020-02-06T00:00:00.000000000'\n",
      " '2020-02-07T00:00:00.000000000' '2020-02-08T00:00:00.000000000'\n",
      " '2020-02-09T00:00:00.000000000' '2020-02-10T00:00:00.000000000']\n",
      "2020-02-11T00:00:00.000000000\n",
      "['2020-01-22T00:00:00.000000000' '2020-01-23T00:00:00.000000000'\n",
      " '2020-01-24T00:00:00.000000000' '2020-01-25T00:00:00.000000000'\n",
      " '2020-01-26T00:00:00.000000000' '2020-01-27T00:00:00.000000000'\n",
      " '2020-01-28T00:00:00.000000000' '2020-01-29T00:00:00.000000000'\n",
      " '2020-01-30T00:00:00.000000000' '2020-01-31T00:00:00.000000000'\n",
      " '2020-02-01T00:00:00.000000000' '2020-02-02T00:00:00.000000000'\n",
      " '2020-02-03T00:00:00.000000000' '2020-02-04T00:00:00.000000000'\n",
      " '2020-02-05T00:00:00.000000000' '2020-02-06T00:00:00.000000000'\n",
      " '2020-02-07T00:00:00.000000000' '2020-02-08T00:00:00.000000000'\n",
      " '2020-02-09T00:00:00.000000000' '2020-02-10T00:00:00.000000000'\n",
      " '2020-02-11T00:00:00.000000000']\n",
      "2020-02-12T00:00:00.000000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_16288\\3429718920.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_data['Cumulative Date'] = i\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_16288\\3429718920.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_data['Cumulative Date'] = i\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_16288\\3429718920.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_data['Cumulative Date'] = i\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_16288\\3429718920.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_data['Cumulative Date'] = i\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_16288\\3429718920.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_data['Cumulative Date'] = i\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_16288\\3429718920.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_data['Cumulative Date'] = i\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_16288\\3429718920.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_data['Cumulative Date'] = i\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_16288\\3429718920.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_data['Cumulative Date'] = i\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_16288\\3429718920.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_data['Cumulative Date'] = i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2020-01-22T00:00:00.000000000' '2020-01-23T00:00:00.000000000'\n",
      " '2020-01-24T00:00:00.000000000' '2020-01-25T00:00:00.000000000'\n",
      " '2020-01-26T00:00:00.000000000' '2020-01-27T00:00:00.000000000'\n",
      " '2020-01-28T00:00:00.000000000' '2020-01-29T00:00:00.000000000'\n",
      " '2020-01-30T00:00:00.000000000' '2020-01-31T00:00:00.000000000'\n",
      " '2020-02-01T00:00:00.000000000' '2020-02-02T00:00:00.000000000'\n",
      " '2020-02-03T00:00:00.000000000' '2020-02-04T00:00:00.000000000'\n",
      " '2020-02-05T00:00:00.000000000' '2020-02-06T00:00:00.000000000'\n",
      " '2020-02-07T00:00:00.000000000' '2020-02-08T00:00:00.000000000'\n",
      " '2020-02-09T00:00:00.000000000' '2020-02-10T00:00:00.000000000'\n",
      " '2020-02-11T00:00:00.000000000' '2020-02-12T00:00:00.000000000']\n",
      "2020-02-13T00:00:00.000000000\n",
      "['2020-01-22T00:00:00.000000000' '2020-01-23T00:00:00.000000000'\n",
      " '2020-01-24T00:00:00.000000000' '2020-01-25T00:00:00.000000000'\n",
      " '2020-01-26T00:00:00.000000000' '2020-01-27T00:00:00.000000000'\n",
      " '2020-01-28T00:00:00.000000000' '2020-01-29T00:00:00.000000000'\n",
      " '2020-01-30T00:00:00.000000000' '2020-01-31T00:00:00.000000000'\n",
      " '2020-02-01T00:00:00.000000000' '2020-02-02T00:00:00.000000000'\n",
      " '2020-02-03T00:00:00.000000000' '2020-02-04T00:00:00.000000000'\n",
      " '2020-02-05T00:00:00.000000000' '2020-02-06T00:00:00.000000000'\n",
      " '2020-02-07T00:00:00.000000000' '2020-02-08T00:00:00.000000000'\n",
      " '2020-02-09T00:00:00.000000000' '2020-02-10T00:00:00.000000000'\n",
      " '2020-02-11T00:00:00.000000000' '2020-02-12T00:00:00.000000000'\n",
      " '2020-02-13T00:00:00.000000000']\n",
      "2020-02-14T00:00:00.000000000\n",
      "['2020-01-22T00:00:00.000000000' '2020-01-23T00:00:00.000000000'\n",
      " '2020-01-24T00:00:00.000000000' '2020-01-25T00:00:00.000000000'\n",
      " '2020-01-26T00:00:00.000000000' '2020-01-27T00:00:00.000000000'\n",
      " '2020-01-28T00:00:00.000000000' '2020-01-29T00:00:00.000000000'\n",
      " '2020-01-30T00:00:00.000000000' '2020-01-31T00:00:00.000000000'\n",
      " '2020-02-01T00:00:00.000000000' '2020-02-02T00:00:00.000000000'\n",
      " '2020-02-03T00:00:00.000000000' '2020-02-04T00:00:00.000000000'\n",
      " '2020-02-05T00:00:00.000000000' '2020-02-06T00:00:00.000000000'\n",
      " '2020-02-07T00:00:00.000000000' '2020-02-08T00:00:00.000000000'\n",
      " '2020-02-09T00:00:00.000000000' '2020-02-10T00:00:00.000000000'\n",
      " '2020-02-11T00:00:00.000000000' '2020-02-12T00:00:00.000000000'\n",
      " '2020-02-13T00:00:00.000000000' '2020-02-14T00:00:00.000000000']\n",
      "2020-02-15T00:00:00.000000000\n",
      "['2020-01-22T00:00:00.000000000' '2020-01-23T00:00:00.000000000'\n",
      " '2020-01-24T00:00:00.000000000' '2020-01-25T00:00:00.000000000'\n",
      " '2020-01-26T00:00:00.000000000' '2020-01-27T00:00:00.000000000'\n",
      " '2020-01-28T00:00:00.000000000' '2020-01-29T00:00:00.000000000'\n",
      " '2020-01-30T00:00:00.000000000' '2020-01-31T00:00:00.000000000'\n",
      " '2020-02-01T00:00:00.000000000' '2020-02-02T00:00:00.000000000'\n",
      " '2020-02-03T00:00:00.000000000' '2020-02-04T00:00:00.000000000'\n",
      " '2020-02-05T00:00:00.000000000' '2020-02-06T00:00:00.000000000'\n",
      " '2020-02-07T00:00:00.000000000' '2020-02-08T00:00:00.000000000'\n",
      " '2020-02-09T00:00:00.000000000' '2020-02-10T00:00:00.000000000'\n",
      " '2020-02-11T00:00:00.000000000' '2020-02-12T00:00:00.000000000'\n",
      " '2020-02-13T00:00:00.000000000' '2020-02-14T00:00:00.000000000'\n",
      " '2020-02-15T00:00:00.000000000']\n",
      "2020-02-16T00:00:00.000000000\n",
      "['2020-01-22T00:00:00.000000000' '2020-01-23T00:00:00.000000000'\n",
      " '2020-01-24T00:00:00.000000000' '2020-01-25T00:00:00.000000000'\n",
      " '2020-01-26T00:00:00.000000000' '2020-01-27T00:00:00.000000000'\n",
      " '2020-01-28T00:00:00.000000000' '2020-01-29T00:00:00.000000000'\n",
      " '2020-01-30T00:00:00.000000000' '2020-01-31T00:00:00.000000000'\n",
      " '2020-02-01T00:00:00.000000000' '2020-02-02T00:00:00.000000000'\n",
      " '2020-02-03T00:00:00.000000000' '2020-02-04T00:00:00.000000000'\n",
      " '2020-02-05T00:00:00.000000000' '2020-02-06T00:00:00.000000000'\n",
      " '2020-02-07T00:00:00.000000000' '2020-02-08T00:00:00.000000000'\n",
      " '2020-02-09T00:00:00.000000000' '2020-02-10T00:00:00.000000000'\n",
      " '2020-02-11T00:00:00.000000000' '2020-02-12T00:00:00.000000000'\n",
      " '2020-02-13T00:00:00.000000000' '2020-02-14T00:00:00.000000000'\n",
      " '2020-02-15T00:00:00.000000000' '2020-02-16T00:00:00.000000000']\n",
      "2020-02-17T00:00:00.000000000\n",
      "['2020-01-22T00:00:00.000000000' '2020-01-23T00:00:00.000000000'\n",
      " '2020-01-24T00:00:00.000000000' '2020-01-25T00:00:00.000000000'\n",
      " '2020-01-26T00:00:00.000000000' '2020-01-27T00:00:00.000000000'\n",
      " '2020-01-28T00:00:00.000000000' '2020-01-29T00:00:00.000000000'\n",
      " '2020-01-30T00:00:00.000000000' '2020-01-31T00:00:00.000000000'\n",
      " '2020-02-01T00:00:00.000000000' '2020-02-02T00:00:00.000000000'\n",
      " '2020-02-03T00:00:00.000000000' '2020-02-04T00:00:00.000000000'\n",
      " '2020-02-05T00:00:00.000000000' '2020-02-06T00:00:00.000000000'\n",
      " '2020-02-07T00:00:00.000000000' '2020-02-08T00:00:00.000000000'\n",
      " '2020-02-09T00:00:00.000000000' '2020-02-10T00:00:00.000000000'\n",
      " '2020-02-11T00:00:00.000000000' '2020-02-12T00:00:00.000000000'\n",
      " '2020-02-13T00:00:00.000000000' '2020-02-14T00:00:00.000000000'\n",
      " '2020-02-15T00:00:00.000000000' '2020-02-16T00:00:00.000000000'\n",
      " '2020-02-17T00:00:00.000000000']\n",
      "2020-02-18T00:00:00.000000000\n",
      "['2020-01-22T00:00:00.000000000' '2020-01-23T00:00:00.000000000'\n",
      " '2020-01-24T00:00:00.000000000' '2020-01-25T00:00:00.000000000'\n",
      " '2020-01-26T00:00:00.000000000' '2020-01-27T00:00:00.000000000'\n",
      " '2020-01-28T00:00:00.000000000' '2020-01-29T00:00:00.000000000'\n",
      " '2020-01-30T00:00:00.000000000' '2020-01-31T00:00:00.000000000'\n",
      " '2020-02-01T00:00:00.000000000' '2020-02-02T00:00:00.000000000'\n",
      " '2020-02-03T00:00:00.000000000' '2020-02-04T00:00:00.000000000'\n",
      " '2020-02-05T00:00:00.000000000' '2020-02-06T00:00:00.000000000'\n",
      " '2020-02-07T00:00:00.000000000' '2020-02-08T00:00:00.000000000'\n",
      " '2020-02-09T00:00:00.000000000' '2020-02-10T00:00:00.000000000'\n",
      " '2020-02-11T00:00:00.000000000' '2020-02-12T00:00:00.000000000'\n",
      " '2020-02-13T00:00:00.000000000' '2020-02-14T00:00:00.000000000'\n",
      " '2020-02-15T00:00:00.000000000' '2020-02-16T00:00:00.000000000'\n",
      " '2020-02-17T00:00:00.000000000' '2020-02-18T00:00:00.000000000']\n",
      "2020-02-19T00:00:00.000000000\n",
      "['2020-01-22T00:00:00.000000000' '2020-01-23T00:00:00.000000000'\n",
      " '2020-01-24T00:00:00.000000000' '2020-01-25T00:00:00.000000000'\n",
      " '2020-01-26T00:00:00.000000000' '2020-01-27T00:00:00.000000000'\n",
      " '2020-01-28T00:00:00.000000000' '2020-01-29T00:00:00.000000000'\n",
      " '2020-01-30T00:00:00.000000000' '2020-01-31T00:00:00.000000000'\n",
      " '2020-02-01T00:00:00.000000000' '2020-02-02T00:00:00.000000000'\n",
      " '2020-02-03T00:00:00.000000000' '2020-02-04T00:00:00.000000000'\n",
      " '2020-02-05T00:00:00.000000000' '2020-02-06T00:00:00.000000000'\n",
      " '2020-02-07T00:00:00.000000000' '2020-02-08T00:00:00.000000000'\n",
      " '2020-02-09T00:00:00.000000000' '2020-02-10T00:00:00.000000000'\n",
      " '2020-02-11T00:00:00.000000000' '2020-02-12T00:00:00.000000000'\n",
      " '2020-02-13T00:00:00.000000000' '2020-02-14T00:00:00.000000000'\n",
      " '2020-02-15T00:00:00.000000000' '2020-02-16T00:00:00.000000000'\n",
      " '2020-02-17T00:00:00.000000000' '2020-02-18T00:00:00.000000000'\n",
      " '2020-02-19T00:00:00.000000000']\n",
      "2020-02-20T00:00:00.000000000\n",
      "['2020-01-22T00:00:00.000000000' '2020-01-23T00:00:00.000000000'\n",
      " '2020-01-24T00:00:00.000000000' '2020-01-25T00:00:00.000000000'\n",
      " '2020-01-26T00:00:00.000000000' '2020-01-27T00:00:00.000000000'\n",
      " '2020-01-28T00:00:00.000000000' '2020-01-29T00:00:00.000000000'\n",
      " '2020-01-30T00:00:00.000000000' '2020-01-31T00:00:00.000000000'\n",
      " '2020-02-01T00:00:00.000000000' '2020-02-02T00:00:00.000000000'\n",
      " '2020-02-03T00:00:00.000000000' '2020-02-04T00:00:00.000000000'\n",
      " '2020-02-05T00:00:00.000000000' '2020-02-06T00:00:00.000000000'\n",
      " '2020-02-07T00:00:00.000000000' '2020-02-08T00:00:00.000000000'\n",
      " '2020-02-09T00:00:00.000000000' '2020-02-10T00:00:00.000000000'\n",
      " '2020-02-11T00:00:00.000000000' '2020-02-12T00:00:00.000000000'\n",
      " '2020-02-13T00:00:00.000000000' '2020-02-14T00:00:00.000000000'\n",
      " '2020-02-15T00:00:00.000000000' '2020-02-16T00:00:00.000000000'\n",
      " '2020-02-17T00:00:00.000000000' '2020-02-18T00:00:00.000000000'\n",
      " '2020-02-19T00:00:00.000000000' '2020-02-20T00:00:00.000000000']\n",
      "2020-02-21T00:00:00.000000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_16288\\3429718920.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_data['Cumulative Date'] = i\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_16288\\3429718920.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_data['Cumulative Date'] = i\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_16288\\3429718920.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_data['Cumulative Date'] = i\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_16288\\3429718920.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_data['Cumulative Date'] = i\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_16288\\3429718920.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_data['Cumulative Date'] = i\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_16288\\3429718920.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_data['Cumulative Date'] = i\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_16288\\3429718920.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_data['Cumulative Date'] = i\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_16288\\3429718920.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_data['Cumulative Date'] = i\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_16288\\3429718920.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_data['Cumulative Date'] = i\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_16288\\3429718920.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_data['Cumulative Date'] = i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2020-01-22T00:00:00.000000000' '2020-01-23T00:00:00.000000000'\n",
      " '2020-01-24T00:00:00.000000000' '2020-01-25T00:00:00.000000000'\n",
      " '2020-01-26T00:00:00.000000000' '2020-01-27T00:00:00.000000000'\n",
      " '2020-01-28T00:00:00.000000000' '2020-01-29T00:00:00.000000000'\n",
      " '2020-01-30T00:00:00.000000000' '2020-01-31T00:00:00.000000000'\n",
      " '2020-02-01T00:00:00.000000000' '2020-02-02T00:00:00.000000000'\n",
      " '2020-02-03T00:00:00.000000000' '2020-02-04T00:00:00.000000000'\n",
      " '2020-02-05T00:00:00.000000000' '2020-02-06T00:00:00.000000000'\n",
      " '2020-02-07T00:00:00.000000000' '2020-02-08T00:00:00.000000000'\n",
      " '2020-02-09T00:00:00.000000000' '2020-02-10T00:00:00.000000000'\n",
      " '2020-02-11T00:00:00.000000000' '2020-02-12T00:00:00.000000000'\n",
      " '2020-02-13T00:00:00.000000000' '2020-02-14T00:00:00.000000000'\n",
      " '2020-02-15T00:00:00.000000000' '2020-02-16T00:00:00.000000000'\n",
      " '2020-02-17T00:00:00.000000000' '2020-02-18T00:00:00.000000000'\n",
      " '2020-02-19T00:00:00.000000000' '2020-02-20T00:00:00.000000000'\n",
      " '2020-02-21T00:00:00.000000000']\n",
      "2020-02-22T00:00:00.000000000\n",
      "['2020-01-22T00:00:00.000000000' '2020-01-23T00:00:00.000000000'\n",
      " '2020-01-24T00:00:00.000000000' '2020-01-25T00:00:00.000000000'\n",
      " '2020-01-26T00:00:00.000000000' '2020-01-27T00:00:00.000000000'\n",
      " '2020-01-28T00:00:00.000000000' '2020-01-29T00:00:00.000000000'\n",
      " '2020-01-30T00:00:00.000000000' '2020-01-31T00:00:00.000000000'\n",
      " '2020-02-01T00:00:00.000000000' '2020-02-02T00:00:00.000000000'\n",
      " '2020-02-03T00:00:00.000000000' '2020-02-04T00:00:00.000000000'\n",
      " '2020-02-05T00:00:00.000000000' '2020-02-06T00:00:00.000000000'\n",
      " '2020-02-07T00:00:00.000000000' '2020-02-08T00:00:00.000000000'\n",
      " '2020-02-09T00:00:00.000000000' '2020-02-10T00:00:00.000000000'\n",
      " '2020-02-11T00:00:00.000000000' '2020-02-12T00:00:00.000000000'\n",
      " '2020-02-13T00:00:00.000000000' '2020-02-14T00:00:00.000000000'\n",
      " '2020-02-15T00:00:00.000000000' '2020-02-16T00:00:00.000000000'\n",
      " '2020-02-17T00:00:00.000000000' '2020-02-18T00:00:00.000000000'\n",
      " '2020-02-19T00:00:00.000000000' '2020-02-20T00:00:00.000000000'\n",
      " '2020-02-21T00:00:00.000000000' '2020-02-22T00:00:00.000000000']\n",
      "2020-02-23T00:00:00.000000000\n",
      "['2020-01-22T00:00:00.000000000' '2020-01-23T00:00:00.000000000'\n",
      " '2020-01-24T00:00:00.000000000' '2020-01-25T00:00:00.000000000'\n",
      " '2020-01-26T00:00:00.000000000' '2020-01-27T00:00:00.000000000'\n",
      " '2020-01-28T00:00:00.000000000' '2020-01-29T00:00:00.000000000'\n",
      " '2020-01-30T00:00:00.000000000' '2020-01-31T00:00:00.000000000'\n",
      " '2020-02-01T00:00:00.000000000' '2020-02-02T00:00:00.000000000'\n",
      " '2020-02-03T00:00:00.000000000' '2020-02-04T00:00:00.000000000'\n",
      " '2020-02-05T00:00:00.000000000' '2020-02-06T00:00:00.000000000'\n",
      " '2020-02-07T00:00:00.000000000' '2020-02-08T00:00:00.000000000'\n",
      " '2020-02-09T00:00:00.000000000' '2020-02-10T00:00:00.000000000'\n",
      " '2020-02-11T00:00:00.000000000' '2020-02-12T00:00:00.000000000'\n",
      " '2020-02-13T00:00:00.000000000' '2020-02-14T00:00:00.000000000'\n",
      " '2020-02-15T00:00:00.000000000' '2020-02-16T00:00:00.000000000'\n",
      " '2020-02-17T00:00:00.000000000' '2020-02-18T00:00:00.000000000'\n",
      " '2020-02-19T00:00:00.000000000' '2020-02-20T00:00:00.000000000'\n",
      " '2020-02-21T00:00:00.000000000' '2020-02-22T00:00:00.000000000'\n",
      " '2020-02-23T00:00:00.000000000']\n",
      "2020-02-24T00:00:00.000000000\n",
      "['2020-01-22T00:00:00.000000000' '2020-01-23T00:00:00.000000000'\n",
      " '2020-01-24T00:00:00.000000000' '2020-01-25T00:00:00.000000000'\n",
      " '2020-01-26T00:00:00.000000000' '2020-01-27T00:00:00.000000000'\n",
      " '2020-01-28T00:00:00.000000000' '2020-01-29T00:00:00.000000000'\n",
      " '2020-01-30T00:00:00.000000000' '2020-01-31T00:00:00.000000000'\n",
      " '2020-02-01T00:00:00.000000000' '2020-02-02T00:00:00.000000000'\n",
      " '2020-02-03T00:00:00.000000000' '2020-02-04T00:00:00.000000000'\n",
      " '2020-02-05T00:00:00.000000000' '2020-02-06T00:00:00.000000000'\n",
      " '2020-02-07T00:00:00.000000000' '2020-02-08T00:00:00.000000000'\n",
      " '2020-02-09T00:00:00.000000000' '2020-02-10T00:00:00.000000000'\n",
      " '2020-02-11T00:00:00.000000000' '2020-02-12T00:00:00.000000000'\n",
      " '2020-02-13T00:00:00.000000000' '2020-02-14T00:00:00.000000000'\n",
      " '2020-02-15T00:00:00.000000000' '2020-02-16T00:00:00.000000000'\n",
      " '2020-02-17T00:00:00.000000000' '2020-02-18T00:00:00.000000000'\n",
      " '2020-02-19T00:00:00.000000000' '2020-02-20T00:00:00.000000000'\n",
      " '2020-02-21T00:00:00.000000000' '2020-02-22T00:00:00.000000000'\n",
      " '2020-02-23T00:00:00.000000000' '2020-02-24T00:00:00.000000000']\n",
      "2020-02-25T00:00:00.000000000\n",
      "['2020-01-22T00:00:00.000000000' '2020-01-23T00:00:00.000000000'\n",
      " '2020-01-24T00:00:00.000000000' '2020-01-25T00:00:00.000000000'\n",
      " '2020-01-26T00:00:00.000000000' '2020-01-27T00:00:00.000000000'\n",
      " '2020-01-28T00:00:00.000000000' '2020-01-29T00:00:00.000000000'\n",
      " '2020-01-30T00:00:00.000000000' '2020-01-31T00:00:00.000000000'\n",
      " '2020-02-01T00:00:00.000000000' '2020-02-02T00:00:00.000000000'\n",
      " '2020-02-03T00:00:00.000000000' '2020-02-04T00:00:00.000000000'\n",
      " '2020-02-05T00:00:00.000000000' '2020-02-06T00:00:00.000000000'\n",
      " '2020-02-07T00:00:00.000000000' '2020-02-08T00:00:00.000000000'\n",
      " '2020-02-09T00:00:00.000000000' '2020-02-10T00:00:00.000000000'\n",
      " '2020-02-11T00:00:00.000000000' '2020-02-12T00:00:00.000000000'\n",
      " '2020-02-13T00:00:00.000000000' '2020-02-14T00:00:00.000000000'\n",
      " '2020-02-15T00:00:00.000000000' '2020-02-16T00:00:00.000000000'\n",
      " '2020-02-17T00:00:00.000000000' '2020-02-18T00:00:00.000000000'\n",
      " '2020-02-19T00:00:00.000000000' '2020-02-20T00:00:00.000000000'\n",
      " '2020-02-21T00:00:00.000000000' '2020-02-22T00:00:00.000000000'\n",
      " '2020-02-23T00:00:00.000000000' '2020-02-24T00:00:00.000000000'\n",
      " '2020-02-25T00:00:00.000000000']\n",
      "2020-02-26T00:00:00.000000000\n",
      "['2020-01-22T00:00:00.000000000' '2020-01-23T00:00:00.000000000'\n",
      " '2020-01-24T00:00:00.000000000' '2020-01-25T00:00:00.000000000'\n",
      " '2020-01-26T00:00:00.000000000' '2020-01-27T00:00:00.000000000'\n",
      " '2020-01-28T00:00:00.000000000' '2020-01-29T00:00:00.000000000'\n",
      " '2020-01-30T00:00:00.000000000' '2020-01-31T00:00:00.000000000'\n",
      " '2020-02-01T00:00:00.000000000' '2020-02-02T00:00:00.000000000'\n",
      " '2020-02-03T00:00:00.000000000' '2020-02-04T00:00:00.000000000'\n",
      " '2020-02-05T00:00:00.000000000' '2020-02-06T00:00:00.000000000'\n",
      " '2020-02-07T00:00:00.000000000' '2020-02-08T00:00:00.000000000'\n",
      " '2020-02-09T00:00:00.000000000' '2020-02-10T00:00:00.000000000'\n",
      " '2020-02-11T00:00:00.000000000' '2020-02-12T00:00:00.000000000'\n",
      " '2020-02-13T00:00:00.000000000' '2020-02-14T00:00:00.000000000'\n",
      " '2020-02-15T00:00:00.000000000' '2020-02-16T00:00:00.000000000'\n",
      " '2020-02-17T00:00:00.000000000' '2020-02-18T00:00:00.000000000'\n",
      " '2020-02-19T00:00:00.000000000' '2020-02-20T00:00:00.000000000'\n",
      " '2020-02-21T00:00:00.000000000' '2020-02-22T00:00:00.000000000'\n",
      " '2020-02-23T00:00:00.000000000' '2020-02-24T00:00:00.000000000'\n",
      " '2020-02-25T00:00:00.000000000' '2020-02-26T00:00:00.000000000']\n",
      "2020-02-27T00:00:00.000000000\n",
      "['2020-01-22T00:00:00.000000000' '2020-01-23T00:00:00.000000000'\n",
      " '2020-01-24T00:00:00.000000000' '2020-01-25T00:00:00.000000000'\n",
      " '2020-01-26T00:00:00.000000000' '2020-01-27T00:00:00.000000000'\n",
      " '2020-01-28T00:00:00.000000000' '2020-01-29T00:00:00.000000000'\n",
      " '2020-01-30T00:00:00.000000000' '2020-01-31T00:00:00.000000000'\n",
      " '2020-02-01T00:00:00.000000000' '2020-02-02T00:00:00.000000000'\n",
      " '2020-02-03T00:00:00.000000000' '2020-02-04T00:00:00.000000000'\n",
      " '2020-02-05T00:00:00.000000000' '2020-02-06T00:00:00.000000000'\n",
      " '2020-02-07T00:00:00.000000000' '2020-02-08T00:00:00.000000000'\n",
      " '2020-02-09T00:00:00.000000000' '2020-02-10T00:00:00.000000000'\n",
      " '2020-02-11T00:00:00.000000000' '2020-02-12T00:00:00.000000000'\n",
      " '2020-02-13T00:00:00.000000000' '2020-02-14T00:00:00.000000000'\n",
      " '2020-02-15T00:00:00.000000000' '2020-02-16T00:00:00.000000000'\n",
      " '2020-02-17T00:00:00.000000000' '2020-02-18T00:00:00.000000000'\n",
      " '2020-02-19T00:00:00.000000000' '2020-02-20T00:00:00.000000000'\n",
      " '2020-02-21T00:00:00.000000000' '2020-02-22T00:00:00.000000000'\n",
      " '2020-02-23T00:00:00.000000000' '2020-02-24T00:00:00.000000000'\n",
      " '2020-02-25T00:00:00.000000000' '2020-02-26T00:00:00.000000000'\n",
      " '2020-02-27T00:00:00.000000000']\n",
      "2020-02-28T00:00:00.000000000\n",
      "['2020-01-22T00:00:00.000000000' '2020-01-23T00:00:00.000000000'\n",
      " '2020-01-24T00:00:00.000000000' '2020-01-25T00:00:00.000000000'\n",
      " '2020-01-26T00:00:00.000000000' '2020-01-27T00:00:00.000000000'\n",
      " '2020-01-28T00:00:00.000000000' '2020-01-29T00:00:00.000000000'\n",
      " '2020-01-30T00:00:00.000000000' '2020-01-31T00:00:00.000000000'\n",
      " '2020-02-01T00:00:00.000000000' '2020-02-02T00:00:00.000000000'\n",
      " '2020-02-03T00:00:00.000000000' '2020-02-04T00:00:00.000000000'\n",
      " '2020-02-05T00:00:00.000000000' '2020-02-06T00:00:00.000000000'\n",
      " '2020-02-07T00:00:00.000000000' '2020-02-08T00:00:00.000000000'\n",
      " '2020-02-09T00:00:00.000000000' '2020-02-10T00:00:00.000000000'\n",
      " '2020-02-11T00:00:00.000000000' '2020-02-12T00:00:00.000000000'\n",
      " '2020-02-13T00:00:00.000000000' '2020-02-14T00:00:00.000000000'\n",
      " '2020-02-15T00:00:00.000000000' '2020-02-16T00:00:00.000000000'\n",
      " '2020-02-17T00:00:00.000000000' '2020-02-18T00:00:00.000000000'\n",
      " '2020-02-19T00:00:00.000000000' '2020-02-20T00:00:00.000000000'\n",
      " '2020-02-21T00:00:00.000000000' '2020-02-22T00:00:00.000000000'\n",
      " '2020-02-23T00:00:00.000000000' '2020-02-24T00:00:00.000000000'\n",
      " '2020-02-25T00:00:00.000000000' '2020-02-26T00:00:00.000000000'\n",
      " '2020-02-27T00:00:00.000000000' '2020-02-28T00:00:00.000000000']\n",
      "2020-02-29T00:00:00.000000000\n",
      "['2020-01-22T00:00:00.000000000' '2020-01-23T00:00:00.000000000'\n",
      " '2020-01-24T00:00:00.000000000' '2020-01-25T00:00:00.000000000'\n",
      " '2020-01-26T00:00:00.000000000' '2020-01-27T00:00:00.000000000'\n",
      " '2020-01-28T00:00:00.000000000' '2020-01-29T00:00:00.000000000'\n",
      " '2020-01-30T00:00:00.000000000' '2020-01-31T00:00:00.000000000'\n",
      " '2020-02-01T00:00:00.000000000' '2020-02-02T00:00:00.000000000'\n",
      " '2020-02-03T00:00:00.000000000' '2020-02-04T00:00:00.000000000'\n",
      " '2020-02-05T00:00:00.000000000' '2020-02-06T00:00:00.000000000'\n",
      " '2020-02-07T00:00:00.000000000' '2020-02-08T00:00:00.000000000'\n",
      " '2020-02-09T00:00:00.000000000' '2020-02-10T00:00:00.000000000'\n",
      " '2020-02-11T00:00:00.000000000' '2020-02-12T00:00:00.000000000'\n",
      " '2020-02-13T00:00:00.000000000' '2020-02-14T00:00:00.000000000'\n",
      " '2020-02-15T00:00:00.000000000' '2020-02-16T00:00:00.000000000'\n",
      " '2020-02-17T00:00:00.000000000' '2020-02-18T00:00:00.000000000'\n",
      " '2020-02-19T00:00:00.000000000' '2020-02-20T00:00:00.000000000'\n",
      " '2020-02-21T00:00:00.000000000' '2020-02-22T00:00:00.000000000'\n",
      " '2020-02-23T00:00:00.000000000' '2020-02-24T00:00:00.000000000'\n",
      " '2020-02-25T00:00:00.000000000' '2020-02-26T00:00:00.000000000'\n",
      " '2020-02-27T00:00:00.000000000' '2020-02-28T00:00:00.000000000'\n",
      " '2020-02-29T00:00:00.000000000']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_16288\\3429718920.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_data['Cumulative Date'] = i\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_16288\\3429718920.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_data['Cumulative Date'] = i\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_16288\\3429718920.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_data['Cumulative Date'] = i\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_16288\\3429718920.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_data['Cumulative Date'] = i\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_16288\\3429718920.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_data['Cumulative Date'] = i\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_16288\\3429718920.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_data['Cumulative Date'] = i\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_16288\\3429718920.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_data['Cumulative Date'] = i\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_16288\\3429718920.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_data['Cumulative Date'] = i\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_16288\\3429718920.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_data['Cumulative Date'] = i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-01T00:00:00.000000000\n",
      "['2020-01-22T00:00:00.000000000' '2020-01-23T00:00:00.000000000'\n",
      " '2020-01-24T00:00:00.000000000' '2020-01-25T00:00:00.000000000'\n",
      " '2020-01-26T00:00:00.000000000' '2020-01-27T00:00:00.000000000'\n",
      " '2020-01-28T00:00:00.000000000' '2020-01-29T00:00:00.000000000'\n",
      " '2020-01-30T00:00:00.000000000' '2020-01-31T00:00:00.000000000'\n",
      " '2020-02-01T00:00:00.000000000' '2020-02-02T00:00:00.000000000'\n",
      " '2020-02-03T00:00:00.000000000' '2020-02-04T00:00:00.000000000'\n",
      " '2020-02-05T00:00:00.000000000' '2020-02-06T00:00:00.000000000'\n",
      " '2020-02-07T00:00:00.000000000' '2020-02-08T00:00:00.000000000'\n",
      " '2020-02-09T00:00:00.000000000' '2020-02-10T00:00:00.000000000'\n",
      " '2020-02-11T00:00:00.000000000' '2020-02-12T00:00:00.000000000'\n",
      " '2020-02-13T00:00:00.000000000' '2020-02-14T00:00:00.000000000'\n",
      " '2020-02-15T00:00:00.000000000' '2020-02-16T00:00:00.000000000'\n",
      " '2020-02-17T00:00:00.000000000' '2020-02-18T00:00:00.000000000'\n",
      " '2020-02-19T00:00:00.000000000' '2020-02-20T00:00:00.000000000'\n",
      " '2020-02-21T00:00:00.000000000' '2020-02-22T00:00:00.000000000'\n",
      " '2020-02-23T00:00:00.000000000' '2020-02-24T00:00:00.000000000'\n",
      " '2020-02-25T00:00:00.000000000' '2020-02-26T00:00:00.000000000'\n",
      " '2020-02-27T00:00:00.000000000' '2020-02-28T00:00:00.000000000'\n",
      " '2020-02-29T00:00:00.000000000' '2020-03-01T00:00:00.000000000']\n",
      "2020-03-02T00:00:00.000000000\n",
      "['2020-01-22T00:00:00.000000000' '2020-01-23T00:00:00.000000000'\n",
      " '2020-01-24T00:00:00.000000000' '2020-01-25T00:00:00.000000000'\n",
      " '2020-01-26T00:00:00.000000000' '2020-01-27T00:00:00.000000000'\n",
      " '2020-01-28T00:00:00.000000000' '2020-01-29T00:00:00.000000000'\n",
      " '2020-01-30T00:00:00.000000000' '2020-01-31T00:00:00.000000000'\n",
      " '2020-02-01T00:00:00.000000000' '2020-02-02T00:00:00.000000000'\n",
      " '2020-02-03T00:00:00.000000000' '2020-02-04T00:00:00.000000000'\n",
      " '2020-02-05T00:00:00.000000000' '2020-02-06T00:00:00.000000000'\n",
      " '2020-02-07T00:00:00.000000000' '2020-02-08T00:00:00.000000000'\n",
      " '2020-02-09T00:00:00.000000000' '2020-02-10T00:00:00.000000000'\n",
      " '2020-02-11T00:00:00.000000000' '2020-02-12T00:00:00.000000000'\n",
      " '2020-02-13T00:00:00.000000000' '2020-02-14T00:00:00.000000000'\n",
      " '2020-02-15T00:00:00.000000000' '2020-02-16T00:00:00.000000000'\n",
      " '2020-02-17T00:00:00.000000000' '2020-02-18T00:00:00.000000000'\n",
      " '2020-02-19T00:00:00.000000000' '2020-02-20T00:00:00.000000000'\n",
      " '2020-02-21T00:00:00.000000000' '2020-02-22T00:00:00.000000000'\n",
      " '2020-02-23T00:00:00.000000000' '2020-02-24T00:00:00.000000000'\n",
      " '2020-02-25T00:00:00.000000000' '2020-02-26T00:00:00.000000000'\n",
      " '2020-02-27T00:00:00.000000000' '2020-02-28T00:00:00.000000000'\n",
      " '2020-02-29T00:00:00.000000000' '2020-03-01T00:00:00.000000000'\n",
      " '2020-03-02T00:00:00.000000000']\n",
      "2020-03-03T00:00:00.000000000\n",
      "['2020-01-22T00:00:00.000000000' '2020-01-23T00:00:00.000000000'\n",
      " '2020-01-24T00:00:00.000000000' '2020-01-25T00:00:00.000000000'\n",
      " '2020-01-26T00:00:00.000000000' '2020-01-27T00:00:00.000000000'\n",
      " '2020-01-28T00:00:00.000000000' '2020-01-29T00:00:00.000000000'\n",
      " '2020-01-30T00:00:00.000000000' '2020-01-31T00:00:00.000000000'\n",
      " '2020-02-01T00:00:00.000000000' '2020-02-02T00:00:00.000000000'\n",
      " '2020-02-03T00:00:00.000000000' '2020-02-04T00:00:00.000000000'\n",
      " '2020-02-05T00:00:00.000000000' '2020-02-06T00:00:00.000000000'\n",
      " '2020-02-07T00:00:00.000000000' '2020-02-08T00:00:00.000000000'\n",
      " '2020-02-09T00:00:00.000000000' '2020-02-10T00:00:00.000000000'\n",
      " '2020-02-11T00:00:00.000000000' '2020-02-12T00:00:00.000000000'\n",
      " '2020-02-13T00:00:00.000000000' '2020-02-14T00:00:00.000000000'\n",
      " '2020-02-15T00:00:00.000000000' '2020-02-16T00:00:00.000000000'\n",
      " '2020-02-17T00:00:00.000000000' '2020-02-18T00:00:00.000000000'\n",
      " '2020-02-19T00:00:00.000000000' '2020-02-20T00:00:00.000000000'\n",
      " '2020-02-21T00:00:00.000000000' '2020-02-22T00:00:00.000000000'\n",
      " '2020-02-23T00:00:00.000000000' '2020-02-24T00:00:00.000000000'\n",
      " '2020-02-25T00:00:00.000000000' '2020-02-26T00:00:00.000000000'\n",
      " '2020-02-27T00:00:00.000000000' '2020-02-28T00:00:00.000000000'\n",
      " '2020-02-29T00:00:00.000000000' '2020-03-01T00:00:00.000000000'\n",
      " '2020-03-02T00:00:00.000000000' '2020-03-03T00:00:00.000000000']\n",
      "2020-03-04T00:00:00.000000000\n",
      "['2020-01-22T00:00:00.000000000' '2020-01-23T00:00:00.000000000'\n",
      " '2020-01-24T00:00:00.000000000' '2020-01-25T00:00:00.000000000'\n",
      " '2020-01-26T00:00:00.000000000' '2020-01-27T00:00:00.000000000'\n",
      " '2020-01-28T00:00:00.000000000' '2020-01-29T00:00:00.000000000'\n",
      " '2020-01-30T00:00:00.000000000' '2020-01-31T00:00:00.000000000'\n",
      " '2020-02-01T00:00:00.000000000' '2020-02-02T00:00:00.000000000'\n",
      " '2020-02-03T00:00:00.000000000' '2020-02-04T00:00:00.000000000'\n",
      " '2020-02-05T00:00:00.000000000' '2020-02-06T00:00:00.000000000'\n",
      " '2020-02-07T00:00:00.000000000' '2020-02-08T00:00:00.000000000'\n",
      " '2020-02-09T00:00:00.000000000' '2020-02-10T00:00:00.000000000'\n",
      " '2020-02-11T00:00:00.000000000' '2020-02-12T00:00:00.000000000'\n",
      " '2020-02-13T00:00:00.000000000' '2020-02-14T00:00:00.000000000'\n",
      " '2020-02-15T00:00:00.000000000' '2020-02-16T00:00:00.000000000'\n",
      " '2020-02-17T00:00:00.000000000' '2020-02-18T00:00:00.000000000'\n",
      " '2020-02-19T00:00:00.000000000' '2020-02-20T00:00:00.000000000'\n",
      " '2020-02-21T00:00:00.000000000' '2020-02-22T00:00:00.000000000'\n",
      " '2020-02-23T00:00:00.000000000' '2020-02-24T00:00:00.000000000'\n",
      " '2020-02-25T00:00:00.000000000' '2020-02-26T00:00:00.000000000'\n",
      " '2020-02-27T00:00:00.000000000' '2020-02-28T00:00:00.000000000'\n",
      " '2020-02-29T00:00:00.000000000' '2020-03-01T00:00:00.000000000'\n",
      " '2020-03-02T00:00:00.000000000' '2020-03-03T00:00:00.000000000'\n",
      " '2020-03-04T00:00:00.000000000']\n",
      "2020-03-05T00:00:00.000000000\n",
      "['2020-01-22T00:00:00.000000000' '2020-01-23T00:00:00.000000000'\n",
      " '2020-01-24T00:00:00.000000000' '2020-01-25T00:00:00.000000000'\n",
      " '2020-01-26T00:00:00.000000000' '2020-01-27T00:00:00.000000000'\n",
      " '2020-01-28T00:00:00.000000000' '2020-01-29T00:00:00.000000000'\n",
      " '2020-01-30T00:00:00.000000000' '2020-01-31T00:00:00.000000000'\n",
      " '2020-02-01T00:00:00.000000000' '2020-02-02T00:00:00.000000000'\n",
      " '2020-02-03T00:00:00.000000000' '2020-02-04T00:00:00.000000000'\n",
      " '2020-02-05T00:00:00.000000000' '2020-02-06T00:00:00.000000000'\n",
      " '2020-02-07T00:00:00.000000000' '2020-02-08T00:00:00.000000000'\n",
      " '2020-02-09T00:00:00.000000000' '2020-02-10T00:00:00.000000000'\n",
      " '2020-02-11T00:00:00.000000000' '2020-02-12T00:00:00.000000000'\n",
      " '2020-02-13T00:00:00.000000000' '2020-02-14T00:00:00.000000000'\n",
      " '2020-02-15T00:00:00.000000000' '2020-02-16T00:00:00.000000000'\n",
      " '2020-02-17T00:00:00.000000000' '2020-02-18T00:00:00.000000000'\n",
      " '2020-02-19T00:00:00.000000000' '2020-02-20T00:00:00.000000000'\n",
      " '2020-02-21T00:00:00.000000000' '2020-02-22T00:00:00.000000000'\n",
      " '2020-02-23T00:00:00.000000000' '2020-02-24T00:00:00.000000000'\n",
      " '2020-02-25T00:00:00.000000000' '2020-02-26T00:00:00.000000000'\n",
      " '2020-02-27T00:00:00.000000000' '2020-02-28T00:00:00.000000000'\n",
      " '2020-02-29T00:00:00.000000000' '2020-03-01T00:00:00.000000000'\n",
      " '2020-03-02T00:00:00.000000000' '2020-03-03T00:00:00.000000000'\n",
      " '2020-03-04T00:00:00.000000000' '2020-03-05T00:00:00.000000000']\n",
      "2020-03-06T00:00:00.000000000\n",
      "['2020-01-22T00:00:00.000000000' '2020-01-23T00:00:00.000000000'\n",
      " '2020-01-24T00:00:00.000000000' '2020-01-25T00:00:00.000000000'\n",
      " '2020-01-26T00:00:00.000000000' '2020-01-27T00:00:00.000000000'\n",
      " '2020-01-28T00:00:00.000000000' '2020-01-29T00:00:00.000000000'\n",
      " '2020-01-30T00:00:00.000000000' '2020-01-31T00:00:00.000000000'\n",
      " '2020-02-01T00:00:00.000000000' '2020-02-02T00:00:00.000000000'\n",
      " '2020-02-03T00:00:00.000000000' '2020-02-04T00:00:00.000000000'\n",
      " '2020-02-05T00:00:00.000000000' '2020-02-06T00:00:00.000000000'\n",
      " '2020-02-07T00:00:00.000000000' '2020-02-08T00:00:00.000000000'\n",
      " '2020-02-09T00:00:00.000000000' '2020-02-10T00:00:00.000000000'\n",
      " '2020-02-11T00:00:00.000000000' '2020-02-12T00:00:00.000000000'\n",
      " '2020-02-13T00:00:00.000000000' '2020-02-14T00:00:00.000000000'\n",
      " '2020-02-15T00:00:00.000000000' '2020-02-16T00:00:00.000000000'\n",
      " '2020-02-17T00:00:00.000000000' '2020-02-18T00:00:00.000000000'\n",
      " '2020-02-19T00:00:00.000000000' '2020-02-20T00:00:00.000000000'\n",
      " '2020-02-21T00:00:00.000000000' '2020-02-22T00:00:00.000000000'\n",
      " '2020-02-23T00:00:00.000000000' '2020-02-24T00:00:00.000000000'\n",
      " '2020-02-25T00:00:00.000000000' '2020-02-26T00:00:00.000000000'\n",
      " '2020-02-27T00:00:00.000000000' '2020-02-28T00:00:00.000000000'\n",
      " '2020-02-29T00:00:00.000000000' '2020-03-01T00:00:00.000000000'\n",
      " '2020-03-02T00:00:00.000000000' '2020-03-03T00:00:00.000000000'\n",
      " '2020-03-04T00:00:00.000000000' '2020-03-05T00:00:00.000000000'\n",
      " '2020-03-06T00:00:00.000000000']\n",
      "2020-03-07T00:00:00.000000000\n",
      "['2020-01-22T00:00:00.000000000' '2020-01-23T00:00:00.000000000'\n",
      " '2020-01-24T00:00:00.000000000' '2020-01-25T00:00:00.000000000'\n",
      " '2020-01-26T00:00:00.000000000' '2020-01-27T00:00:00.000000000'\n",
      " '2020-01-28T00:00:00.000000000' '2020-01-29T00:00:00.000000000'\n",
      " '2020-01-30T00:00:00.000000000' '2020-01-31T00:00:00.000000000'\n",
      " '2020-02-01T00:00:00.000000000' '2020-02-02T00:00:00.000000000'\n",
      " '2020-02-03T00:00:00.000000000' '2020-02-04T00:00:00.000000000'\n",
      " '2020-02-05T00:00:00.000000000' '2020-02-06T00:00:00.000000000'\n",
      " '2020-02-07T00:00:00.000000000' '2020-02-08T00:00:00.000000000'\n",
      " '2020-02-09T00:00:00.000000000' '2020-02-10T00:00:00.000000000'\n",
      " '2020-02-11T00:00:00.000000000' '2020-02-12T00:00:00.000000000'\n",
      " '2020-02-13T00:00:00.000000000' '2020-02-14T00:00:00.000000000'\n",
      " '2020-02-15T00:00:00.000000000' '2020-02-16T00:00:00.000000000'\n",
      " '2020-02-17T00:00:00.000000000' '2020-02-18T00:00:00.000000000'\n",
      " '2020-02-19T00:00:00.000000000' '2020-02-20T00:00:00.000000000'\n",
      " '2020-02-21T00:00:00.000000000' '2020-02-22T00:00:00.000000000'\n",
      " '2020-02-23T00:00:00.000000000' '2020-02-24T00:00:00.000000000'\n",
      " '2020-02-25T00:00:00.000000000' '2020-02-26T00:00:00.000000000'\n",
      " '2020-02-27T00:00:00.000000000' '2020-02-28T00:00:00.000000000'\n",
      " '2020-02-29T00:00:00.000000000' '2020-03-01T00:00:00.000000000'\n",
      " '2020-03-02T00:00:00.000000000' '2020-03-03T00:00:00.000000000'\n",
      " '2020-03-04T00:00:00.000000000' '2020-03-05T00:00:00.000000000'\n",
      " '2020-03-06T00:00:00.000000000' '2020-03-07T00:00:00.000000000']\n",
      "(243000, 15)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_16288\\3429718920.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_data['Cumulative Date'] = i\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_16288\\3429718920.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_data['Cumulative Date'] = i\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_16288\\3429718920.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_data['Cumulative Date'] = i\n"
     ]
    }
   ],
   "source": [
    "# puting unique values in list\n",
    "dates = full_join3['Date'].unique()\n",
    "\n",
    "#creating a df with unique\n",
    "dates = pd.DataFrame(dates, columns=['Date'])\n",
    "\n",
    "# Ordering df\n",
    "dates = dates.sort_values(by=['Date'])\n",
    "\n",
    "# Creating an ordered list now\n",
    "dates = full_join3['Date'].unique()\n",
    "\n",
    "print(full_join3.shape)\n",
    "\n",
    "try:\n",
    "    del concat_data\n",
    "except:\n",
    "    print()\n",
    "    \n",
    "try:\n",
    "    del final_concat_data\n",
    "except:\n",
    "    print()\n",
    "    \n",
    "\n",
    "for i in dates:\n",
    "    new_data = full_join3[full_join3['Date'] == i]\n",
    "    new_data['Cumulative Date'] = i\n",
    "    print(i)\n",
    "    \n",
    "    try:     \n",
    "        concat_data = pd.concat([concat_data, new_data], ignore_index = True)\n",
    "        concat_data['Cumulative Date 2'] = i\n",
    "        print(concat_data['Date'].unique())\n",
    "        \n",
    "        try:\n",
    "            final_concat_data = pd.concat([final_concat_data, concat_data], ignore_index = True)\n",
    "        except:\n",
    "            final_concat_data = concat_data\n",
    "\n",
    "    except:\n",
    "        concat_data = new_data\n",
    "        \n",
    "print(final_concat_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7c981494",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_concat_data.to_csv('CoronaVirus PowerBI Raw - Cumulative Test', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f4413d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
